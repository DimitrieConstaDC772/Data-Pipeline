# End-to-End Data Pipeline and Dashboard Project

This repository showcases a complete data workflow that includes **Python ETL scripts**, **Airflow orchestration**, **dbt transformations**, **Docker containerization**, and a **Power BI dashboard** for visualization.

---

## ğŸ“ Project Structure
-
project-root/
â”œâ”€â”€ dags/ # Airflow DAG files
â”œâ”€â”€ dbt/ # dbt project
â”‚ â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ dbt_project.yml
â”‚ â””â”€â”€ ...
â”œâ”€â”€ scripts/ # Python ETL scripts
â”œâ”€â”€ plugins/ # Airflow plugins
â”œâ”€â”€ docker/ # Docker and docker-compose files
â”œâ”€â”€ requirements/ # Python dependencies
â”œâ”€â”€ reports/ # Power BI dashboard
â”‚ â”œâ”€â”€ dashboard.pbix
â”‚ â””â”€â”€ screenshots/
â”œâ”€â”€ .env.example # Example environment variables
â””â”€â”€ README.md
---

## Technologies Used

- **Python** â€“ ETL scripts and data processing
- **Airflow** â€“ DAG orchestration for automated pipelines
- **dbt** â€“ Data transformations and modeling
- **Docker & docker-compose** â€“ Containerized development environment
- **Power BI** â€“ Interactive dashboards for reporting

---

## Setup Instructions

1. **Clone the repository**

```bash
git clone https://github.com/DimitrieConstaDC772/Data-Pipeline.git
cd your-repo


2. Create your .env file
Copy .env.example and fill in your local paths and Airflow Fernet key:

cp .env.example .env
# Then edit .env to add your local secrets paths and Fernet key

3. Start the environment with Docker

docker-compose up


Airflow webserver will be available at http://localhost:8080.

DAGs will automatically load from dags/.

dbt models are mounted into the container and can be run inside Airflow tasks.

4. Power BI Dashboard

Open reports/dashboard.pbix in Power BI Desktop.

Use the screenshots in reports/screenshots/ as a preview of the pages
