# End-to-End Data Pipeline and Dashboard Project

This repository showcases a complete data workflow that includes **Python ETL scripts**, **Airflow orchestration**, **dbt transformations**, **Docker containerization**, and a **Power BI dashboard** for visualization.

---

## ğŸ“ Project Structure
project-root/
â”œâ”€â”€ dags/ # Airflow DAG files
â”œâ”€â”€ dbt/ # dbt project
â”‚ â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ dbt_project.yml
â”‚ â””â”€â”€ ...
â”œâ”€â”€ scripts/ # Python ETL scripts
â”œâ”€â”€ plugins/ # Airflow plugins
â”œâ”€â”€ docker/ # Docker and docker-compose files
â”œâ”€â”€ requirements/ # Python dependencies
â”œâ”€â”€ reports/ # Power BI dashboard
â”‚ â”œâ”€â”€ dashboard.pbix
â”‚ â””â”€â”€ screenshots/
â”œâ”€â”€ .env.example # Example environment variables
â””â”€â”€ README.md
---

## Technologies Used

- **Python** â€“ ETL scripts and data processing
- **Airflow** â€“ DAG orchestration for automated pipelines
- **dbt** â€“ Data transformations and modeling
- **Docker & docker-compose** â€“ Containerized development environment
- **Power BI** â€“ Interactive dashboards for reporting

---

setup_instructions:
  step_1:
    title: "Clone the repository"
    description: "Get a local copy of the project."
    instructions:
      - "Clone the repository to your local machine."
      - "Navigate into the project folder."
    commands:
      - "git clone https://github.com/yourusername/your-repo.git"
      - "cd your-repo"

  step_2:
    title: "Create your .env file"
    description: "Set up environment variables for local development."
    instructions:
      - "Copy the example `.env` file to create your own `.env`."
      - "Edit `.env` to add your local secrets paths and Airflow Fernet key."
    commands:
      - "cp .env.example .env"
      - "nano .env   # or open with your preferred editor to add secrets"

  step_3:
    title: "Start the environment with Docker"
    description: "Run the full containerized environment."
    instructions:
      - "Start Docker containers using docker-compose."
      - "Airflow webserver will be available at http://localhost:8080."
      - "DAGs will automatically load from the `dags/` folder."
      - "dbt models are mounted into the container and can be run inside Airflow tasks."
    commands:
      - "docker-compose up"

  step_4:
    title: "Power BI Dashboard"
    description: "Access the reporting dashboard."
    instructions:
      - "Open `reports/dashboard.pbix` in Power BI Desktop."
      - "Use the screenshots in `reports/screenshots/` as a preview of the dashboard pages."
    commands: []

  step_4:
    description: "Power BI Dashboard"
    instructions:
      - "Open reports/dashboard.pbix in Power BI Desktop"
      - "Use the screenshots in reports/screenshots/ as a preview of the pages"
