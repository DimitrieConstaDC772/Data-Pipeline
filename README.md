# End-to-End Data Pipeline and Dashboard Project

This repository showcases a complete data workflow that includes **Python ETL scripts**, **Airflow orchestration**, **dbt transformations**, **Docker containerization**, and a **Power BI dashboard** for visualization.

---

## ğŸ“ Project Structure
project-root/
â”œâ”€â”€ dags/ # Airflow DAG files
â”œâ”€â”€ dbt/ # dbt project
â”‚ â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ dbt_project.yml
â”‚ â””â”€â”€ ...
â”œâ”€â”€ scripts/ # Python ETL scripts
â”œâ”€â”€ plugins/ # Airflow plugins
â”œâ”€â”€ docker/ # Docker and docker-compose files
â”œâ”€â”€ requirements/ # Python dependencies
â”œâ”€â”€ reports/ # Power BI dashboard
â”‚ â”œâ”€â”€ dashboard.pbix
â”‚ â””â”€â”€ screenshots/
â”œâ”€â”€ .env.example # Example environment variables
â””â”€â”€ README.md
---

## Technologies Used

- **Python** â€“ ETL scripts and data processing
- **Airflow** â€“ DAG orchestration for automated pipelines
- **dbt** â€“ Data transformations and modeling
- **Docker & docker-compose** â€“ Containerized development environment
- **Power BI** â€“ Interactive dashboards for reporting

---

setup_instructions:
  step_1:
    description: "Clone the repository"
    commands:
      - "git clone https://github.com/yourusername/your-repo.git"
      - "cd your-repo"

  step_2:
    description: "Create your .env file"
    instructions:
      - "Copy .env.example to .env"
      - "Edit .env to add your local secrets paths and Airflow Fernet key"
    commands:
      - "cp .env.example .env"

  step_3:
    description: "Start the environment with Docker"
    commands:
      - "docker-compose up"
    notes:
      - "Airflow webserver will be available at http://localhost:8080"
      - "DAGs will automatically load from dags/"
      - "dbt models are mounted into the container and can be run inside Airflow tasks"

  step_4:
    description: "Power BI Dashboard"
    instructions:
      - "Open reports/dashboard.pbix in Power BI Desktop"
      - "Use the screenshots in reports/screenshots/ as a preview of the pages"
